[{"categories":null,"content":"RL Problem ","date":"2022-06-08","objectID":"/rl-problem/:0:0","tags":null,"title":"Deep Reinforcement Learning: An overview","uri":"/rl-problem/"},{"categories":null,"content":"Reinforcement Learning Overview This section provides background of RL briefly. ","date":"2022-06-08","objectID":"/rl-problem/:1:0","tags":null,"title":"Deep Reinforcement Learning: An overview","uri":"/rl-problem/"},{"categories":null,"content":"Problem setup One biggest difference between RL and other machine learning problem is that the dataset is generated in a very different way. We first consider the components that are included in a typical RL problem: Agent: The agent serves as the subject of learning. It’s the one that conducts the learning behavior and the receiver of the consequences. It interacts with an environment overtime. Action: Actions are the behavior which forms the interaction between agent and environment. The action space $A$ serves as a set of all the actions that can be taken by the agent. State: States model the environment. The agent typicall fires the action based on the state it’s in. Policy: A policy $\\pi$ is normally a mapping between state $s_t$ and action $a_t$. It’s modeled as a probablity distribution for the action in every single state. Reward: Reward is a signal generated when the agent interacts with the environment. It’s the most important standard for the agent to evaluate the learning process. In another word, it seeks to maximize the expectation of accumulated reward: $max R_t = \\sum_{k=0}^\\infty \\gamma^k r_{t+k}$ where $\\gamma$ is the discount rate With the above components and a simulated environment, the agent will perform action based on its current state. After an action is performed, reward is generated as a feedback to the agent so that it can use it to define what should/should not be learned. The dataset, as depicted by the last paragraph, is generated by the agent’s actions and environment. Compared to supervised learning, the dataset in RL is less structured and possibly biased (think about the states that will not be reached and the actions that never be performed). ","date":"2022-06-08","objectID":"/rl-problem/:1:1","tags":null,"title":"Deep Reinforcement Learning: An overview","uri":"/rl-problem/"},{"categories":null,"content":"Prediction Problem and Control Problem ","date":"2022-06-08","objectID":"/rl-problem/:2:0","tags":null,"title":"Deep Reinforcement Learning: An overview","uri":"/rl-problem/"},{"categories":null,"content":"Value Function Before actually talking about the prediction problem and the control problem, we need to first go through Value Function since the major difference of those 2 problems is in the way they make use of the value function. The definition of the value function can be defined above each state or each state-action pair, this is known as the state value function and the action value function. The state value function: $$ v_\\pi (s) = E[R_t | s_t=s] $$ which is the expected return of state $s$ following policy $\\pi$ The action value function: $$ q_\\pi(s, a) = E[R_t|s_t=s, a_t=a] $$ which is similar to how state value is defined but also specified the action taken. We can decompose the above two into Bellman Equation: $$ v_\\pi(s) = \\sum_{a \\in A}\\pi(a|s)\\sum_{s’, r}p(s’, r | s, a)[r + \\gamma v_\\pi(s’)] $$ That is, for each transition $(s, s’)$, we sum up the product of the probability of transition from $s$ to $s’$ by taking action $a$ and times it to the probability of taking action $a$ if in state $s$ $$ q_\\pi(s, a) = \\sum_{s’, r} p(s’, r | s, a)[r+\\gamma \\sum_{a’} \\pi(a’ | s’)q_\\pi(s’, a’)] $$ The optimal value of the above equation can be obtained by: $$ v^(s) = \\max_\\pi v_\\pi(s) = \\max_a q_{\\pi}^(s, a) $$ ","date":"2022-06-08","objectID":"/rl-problem/:2:1","tags":null,"title":"Deep Reinforcement Learning: An overview","uri":"/rl-problem/"},{"categories":null,"content":"Intro Hi, welcome to the first blog of this site. As mentioned in the home info, this is a static blog site hosted on github pages, purely for documenting the learning outcome of various fields in my study. The majority of the blogs focuses on Deep Reinforcement Learning. The idea post should include the classic methods and mathematical formulas, thought process, application in recent researches, and code implementation. ","date":"2022-06-04","objectID":"/my-first-post/:1:0","tags":null,"title":"Hello World!","uri":"/my-first-post/"},{"categories":null,"content":"Acknowledgement I would like to thank all of my friends who make significant contributions for the page (including picking out the theme, and helping with the content of posts): Friend 1 ","date":"2022-06-04","objectID":"/my-first-post/:2:0","tags":null,"title":"Hello World!","uri":"/my-first-post/"}]