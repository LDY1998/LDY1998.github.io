---
title: "Deep Reinforcement Learning: An overview"
date: 2022-06-08T12:26:35-07:00
author: "Jerry Liu"
---

# RL Problem

## Reinforcement Learning Overview

This section provides background of RL briefly. 

### Problem setup

One biggest difference between *RL* and other *machine learning* problem is that the dataset is generated in a very different way.

We first consider the components that are included in a typical RL problem:

- Agent: The agent serves as the subject of learning. It's the one that conducts the learning behavior and the receiver of the consequences. It interacts with an environment overtime.
- Action: Actions are the behavior which forms the interaction between agent and environment. The action space $A$ serves as a set of all the actions that can be taken by the agent.
- State: States model the environment. The agent typicall fires the action based on the state it's in. 
- Policy: A policy $\pi$ is normally a mapping between state $s_t$ and action $a_t$. It's modeled as a probablity distribution for the action in every single state.
- Reward: Reward is a signal generated when the agent interacts with the environment. It's the most important standard for the agent to evaluate the learning process. In another word, it seeks to maximize the expectation of accumulated reward: $max  R_t = \sum_{k=0}^\infty \gamma^k r_{t+k}$ where $\gamma$ is the discount rate

With the above components and a simulated environment, the **agent** will perform **action** based on its current **state**. After an **action** is performed, **reward** is generated as a feedback to the agent so that it can use it to define what should/should not be learned.

The dataset, as depicted by the last paragraph, is generated by the agent's actions and environment. Compared to supervised learning, the dataset in **RL** is less structured and possibly biased (think about the states that will not be reached and the actions that never be performed).

## Prediction Problem and Control Problem

### Value Function

Before actually talking about the **prediction problem** and the **control problem**, we need to first go through **Value Function** since the major difference of those 2 problems is in the way they make use of the value function.

The definition of the value function can be defined above each state or each **state-action pair**, this is known as the **state value function** and the **action value function**.

The state value function:
$$
v_\pi (s) = E[R_t | s_t=s]
$$
which is the expected return of state $s$ following policy $\pi$

The action value function:
$$
    q_\pi(s, a) = E[R_t|s_t=s, a_t=a]
$$
which is similar to how state value is defined but also specified the action taken.

We can decompose the above two into Bellman Equation:
$$
    v_\pi(s) = \sum_{a \in A}\pi(a|s)\sum_{s', r}p(s', r | s, a)[r + \gamma v_\pi(s')]
$$
That is, for each transition $(s, s')$, we sum up the product of the probability of transition from $s$ to $s'$ by taking action $a$ and times it to the probability of taking action $a$ if in state $s$

$$
    q_\pi(s, a) = \sum_{s', r} p(s', r | s, a)[r+\gamma \sum_{a'} \pi(a' | s')q_\pi(s', a')]
$$

The optimal value of the above equation can be obtained by:

$$
    v^*(s) = \max_\pi v_\pi(s) = \max_a q_{\pi}^*(s, a)
$$